{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample program for training NN for automatic driving of game character given sampled screen images and key presses\n",
    "\n",
    "#import initial libraries\n",
    "import cv2\n",
    "print(\"OpenCV: \")\n",
    "print(cv2.__version__)\n",
    "\n",
    "from datetime import datetime\n",
    "from IPython import display\n",
    "from IPython.display import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "home_folder = \"/home/user0/autodrive-game/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add log samples to load\n",
    "\n",
    "log_list =[]\n",
    "log_list.append(\"2019-11-10T04-22-51.961927Z\")\n",
    "\n",
    "#set resolution of samples for training\n",
    "res_x = 124\n",
    "res_y = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the samples, resize them and assign their categories (0=IDLE, 1=ACCEL, 2=LEFT, 3=RIGHT, 4=LEFT+ACCEL, 5=RIGHT+ACCEL)\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "x_total = []\n",
    "y_total = []\n",
    "\n",
    "#create a plot\n",
    "res = plt.figure(figsize = (5,5))\n",
    "ax1 = res.add_subplot(2,1,1)\n",
    "\n",
    "for curr_file in log_list:\n",
    "    DIR = home_folder+\"imgs/\"+curr_file\n",
    "    num_files = len([name for name in os.listdir(DIR) if os.path.isfile(os.path.join(DIR, name))])\n",
    "    print(num_files)\n",
    "\n",
    "    df = pd.read_csv(curr_file+\".csv\",header=None)\n",
    "\n",
    "    for imgcounter in range(2,num_files-1):\n",
    "        #print (df.loc[imgcounter1,:])\n",
    "\n",
    "        img2 = cv2.imread(home_folder + ('imgs/'+curr_file+'/%06d.jpg' % (imgcounter)), cv2.IMREAD_COLOR)\n",
    "        height, width, depth = img2.shape\n",
    "        img2 = cv2.resize(img2, (res_x, res_y))\n",
    "        img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        img1 = cv2.imread(home_folder + ('imgs/'+curr_file+'/%06d.jpg' % (imgcounter-1)), cv2.IMREAD_COLOR)\n",
    "        height, width, depth = img1.shape\n",
    "        img1 = cv2.resize(img1, (res_x, res_y))\n",
    "        img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        img0 = cv2.imread(home_folder + ('imgs/'+curr_file+'/%06d.jpg' % (imgcounter-2)), cv2.IMREAD_COLOR)\n",
    "        height, width, depth = img0.shape\n",
    "        img0 = cv2.resize(img0, (res_x, res_y))\n",
    "        img0 = cv2.cvtColor(img0, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        h0, w0 = img0.shape[:2]\n",
    "        h1, w1 = img1.shape[:2]\n",
    "        h2, w2 = img2.shape[:2]\n",
    "\n",
    "        #create empty matrix\n",
    "        finalimg = np.zeros((max(h0, h1, h2), w0+w1+w2,3), np.uint8)\n",
    "\n",
    "        #combine images\n",
    "        finalimg[:h0, :w0,:3] = img0\n",
    "        finalimg[:h1, w0:w0+w1,:3] = img1\n",
    "        finalimg[:h2, w0+w1:w0+w1+w2,:3] = img2\n",
    "        \n",
    "        x_total.append(finalimg.copy())\n",
    "\n",
    "        if(df.loc[imgcounter,1]==0 and df.loc[imgcounter,4] ==0 and df.loc[imgcounter,5]==0):\n",
    "            y_total.append(0)#idle\n",
    "            print(\"IDLE\")\n",
    "        elif(df.loc[imgcounter,1]==1 and df.loc[imgcounter,4] ==0 and df.loc[imgcounter,5]==0):\n",
    "            y_total.append(1)#accel only\n",
    "            print(\"ACCEL\")\n",
    "        elif(df.loc[imgcounter,1]==0 and df.loc[imgcounter,4] ==1 and df.loc[imgcounter,5]==0):\n",
    "            y_total.append(2)#left no accel\n",
    "            print(\"LEFT\")\n",
    "        elif(df.loc[imgcounter,1]==0 and df.loc[imgcounter,4] ==0 and df.loc[imgcounter,5]==1):\n",
    "            y_total.append(3)#right no accel\n",
    "            print(\"RIGHT\")\n",
    "        elif(df.loc[imgcounter,1]==1 and df.loc[imgcounter,4] ==1 and df.loc[imgcounter,5]==0):\n",
    "            y_total.append(4)#left accel\n",
    "            print(\"LEFT + ACCEL\")\n",
    "        elif(df.loc[imgcounter,1]==1 and df.loc[imgcounter,4] ==0 and df.loc[imgcounter,5]==1):\n",
    "            y_total.append(5)#right accel\n",
    "            print(\"RIGHT + ACCEL\")\n",
    "        else:\n",
    "            y_total.append(0)#idle\n",
    "            print(\"IDLE\")\n",
    "\n",
    "        #inspect the progress once in a while\n",
    "        if(imgcounter%100==0):\n",
    "            ax1.imshow(x_total[imgcounter-2])\n",
    "            plt.draw()\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(plt.gcf())\n",
    "            ax1.cla()\n",
    "   \n",
    "        \n",
    "print(\"samples shape: \", np.array(x_total).shape)\n",
    "print(\"categories shape: \", np.array(y_total).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle the samples so they are in random order\n",
    "def sample_shuffle(a, b):\n",
    "    npa = np.array(a)\n",
    "    npb = np.array(b)\n",
    "    assert len(a) == len(b)\n",
    "    shuffled_a = np.empty(npa.shape, dtype=npa.dtype)\n",
    "    shuffled_b = np.empty(npb.shape, dtype=npb.dtype)\n",
    "    permutation = np.random.permutation(len(npa))\n",
    "    for old_index, new_index in enumerate(permutation):\n",
    "        shuffled_a[new_index] = npa[old_index]\n",
    "        shuffled_b[new_index] = npb[old_index]\n",
    "    return shuffled_a, shuffled_b\n",
    "\n",
    "x_random, y_random = sample_shuffle(x_total,y_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if the samples were shuffled properly with their correct labels\n",
    "\n",
    "randomnum = random.randint(0,len(y_random)-1)\n",
    "print(randomnum)\n",
    "\n",
    "#create a plot\n",
    "res = plt.figure(figsize = (5,5))\n",
    "ax1 = res.add_subplot(2,1,1)\n",
    "\n",
    "ax1.imshow(x_random[randomnum])\n",
    "print(\"sample_random: \", np.array(x_random[randomnum]).shape)\n",
    "print(\"category label: \", y_random[randomnum])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the array with the samples to have one set for training and one set for validation\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for trainsample in range(0,int(len(y_random)*0.7)):#70% for training\n",
    "    x_train.append(x_random[trainsample])\n",
    "    y_train.append(y_random[trainsample])\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "for testsample in range(int(len(y_random)*0.7),len(y_random)-1):#30% for validation\n",
    "    x_test.append(x_random[testsample])\n",
    "    y_test.append(y_random[testsample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the format of the array for inserting it into the NN\n",
    "\n",
    "x_train_np = np.array(x_train)\n",
    "y_train_np = np.array(y_train)\n",
    "\n",
    "x_test_np = np.array(x_test)\n",
    "y_test_np = np.array(y_test)\n",
    "\n",
    "print(x_train_np.shape)\n",
    "print(y_train_np.shape)\n",
    "print(x_test_np.shape)\n",
    "print(y_test_np.shape)\n",
    "\n",
    "print(x_test_np.dtype)\n",
    "print(y_test_np.dtype)\n",
    "\n",
    "print(x_test_np.min())\n",
    "print(x_test_np.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjusting the shape of the array with the data set\n",
    "\n",
    "if len(x_train_np.shape) != 4:\n",
    "    x_train_np = np.expand_dims(x_train_np, axis=3)\n",
    "if len(x_test_np.shape) != 4:\n",
    "    x_test_np = np.expand_dims(x_test_np, axis=3)\n",
    "\n",
    "#adjust the range of values into 0.0 to 1.0\n",
    "x_train_np, x_test_np = x_train_np.astype('float') / 255, x_test_np.astype('float') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the NN libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.models import model_from_json\n",
    "from keras import layers\n",
    "from keras.callbacks import Callback\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from itertools import groupby\n",
    "import os\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the format of the labels into one-hot (multiple categories)\n",
    "\n",
    "y_train_onehot, y_test_onehot = to_categorical(y_train_np), to_categorical(y_test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.inf)\n",
    "print(y_test_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check format of data set\n",
    "print(x_train_np.shape)\n",
    "print(y_train_onehot.shape)\n",
    "print(x_test_np.shape)\n",
    "print(y_test_onehot.shape)\n",
    "\n",
    "print(x_test_np.dtype)\n",
    "print(y_test_onehot.dtype)\n",
    "\n",
    "print(x_test_np.min())\n",
    "print(x_test_np.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the NN\n",
    "\n",
    "def NN_model():\n",
    "    model = Sequential()\n",
    "    model.add(layers.BatchNormalization(name='InputLayer_0',\n",
    "                                        input_shape=(res_y, res_x*3, 3)))\n",
    "    model.add(layers.Dropout(name='DropoutLayer_0', rate=0.2))\n",
    "\n",
    "    model.add(layers.Conv2D(name='CNNLayer_0',\n",
    "                            filters=32,\n",
    "                            kernel_size=(3, 3),\n",
    "                            activation='relu',\n",
    "                            border_mode=\"same\"))\n",
    "    model.add(layers.MaxPooling2D(name='MaxPool_0'))\n",
    "    model.add(layers.Flatten(name='Flat_0'))\n",
    "    model.add(layers.Dropout(rate=0.5))\n",
    "\n",
    "    model.add(layers.Dense(name='DenseLayer_0', units=1024))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.Dropout(rate=0.2))\n",
    "    \n",
    "    model.add(layers.Dense(name='DenseLayer_1', units=1024))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.Dropout(rate=0.6))\n",
    "    \n",
    "    model.add(layers.Dense(name='DenseLayer_2', units=1024))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.Dropout(rate=0.5))\n",
    "    \n",
    "    model.add(layers.Dense(name='DenseLayer_3', units=512))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.Dropout(rate=0.25))\n",
    "    \n",
    "    model.add(layers.Dense(name='DenseLayer_4', units=512))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.Dropout(rate=0.5))\n",
    "    \n",
    "    model.add(layers.Dense(name='DenseLayer_5', units=256))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.Dropout(rate=0.3))\n",
    "    \n",
    "    model.add(layers.Dense(name='DenseLayer_6', units=128))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.Dropout(rate=0.2))\n",
    "    \n",
    "    model.add(layers.Dense(name='DenseLayer_7', units=32))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.Dropout(rate=0.1))\n",
    "\n",
    "    model.add(layers.Dense(name='OutputLayer', units=6))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define helper functions and callbacks to track the progress of the NN training\n",
    "\n",
    "val_f1s = []\n",
    "val_recalls = []\n",
    "val_precisions = []\n",
    "val_x_axis = []\n",
    "\n",
    "def set_seed(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = '%d' % (seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.set_random_seed(seed)\n",
    "\n",
    "def plot_training_summary(training_summary, time_summary=None):\n",
    "    if time_summary:\n",
    "        print(\"Training time (sec):\")\n",
    "        print(time_summary.training_time)\n",
    "        print(\"Epoch times (sec):\")\n",
    "        print(time_summary.epoch_times)\n",
    "    hist = sorted(training_summary.history.items(),\n",
    "                  key=lambda x: (x[0].replace('val_', ''), x[0]))\n",
    "\n",
    "    epochs = [e + 1 for e in training_summary.epoch]\n",
    "    for metric, values in groupby(hist,\n",
    "                                  key=lambda x: x[0].replace('val_', '')):\n",
    "        if 'val_loss' in training_summary.history:\n",
    "            val0, val1 = tuple(values)\n",
    "            plt.plot(epochs, val0[1], epochs, val1[1], '--', marker='o')\n",
    "        else:\n",
    "            val0 = tuple(values)[0]\n",
    "            plt.plot(epochs, val0[1], '--', marker='o')\n",
    "        plt.xlabel('epoch'), plt.ylabel(val0[0])\n",
    "        plt.legend(('Train set', 'Validation set'))\n",
    "        plt.show()\n",
    "        \n",
    "    val_x_axis = range(0,len(val_f1s))\n",
    "    val_classes = range(0,len(val_f1s[0]))\n",
    "    \n",
    "    val_f1_plot = []\n",
    "    for idx_class in val_classes:\n",
    "        val_f1_plot.append([])\n",
    "        for idx_epoch in val_x_axis:\n",
    "            val_f1_plot[idx_class].append(val_f1s[idx_epoch][idx_class])\n",
    "    \n",
    "    for idx_class in val_classes:\n",
    "        plt.plot(val_x_axis,val_f1_plot[idx_class], '--', marker='o')\n",
    "        plt.xlabel('epoch'), plt.ylabel('f1_score class %d' % idx_class)\n",
    "        plt.show()\n",
    "\n",
    "class TimeSummary(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.epoch_times = []\n",
    "        self.training_time = time.process_time()\n",
    "        #for precision, recall and f1 score\n",
    "        val_f1s = []\n",
    "        val_recalls = []\n",
    "        val_precisions = []\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        self.training_time = time.process_time() - self.training_time\n",
    "\n",
    "    def on_epoch_begin(self, batch, logs={}):\n",
    "        self.epoch_time_start = time.process_time()\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.epoch_times.append(time.process_time() - self.epoch_time_start)\n",
    "        #for precision, recall and f1 score\n",
    "        val_predict = (np.asarray(self.model.predict(self.validation_data[0]))).round()\n",
    "        val_targ = self.validation_data[1]\n",
    "        _val_f1 = f1_score(val_targ, val_predict, average=None)\n",
    "        _val_recall = recall_score(val_targ, val_predict, average=None)\n",
    "        _val_precision = precision_score(val_targ, val_predict, average=None)\n",
    "        val_f1s.append(_val_f1)\n",
    "        val_recalls.append(_val_recall)\n",
    "        val_precisions.append(_val_precision)\n",
    "        print(\" - val_f1 (per class):\")\n",
    "        print(_val_f1)\n",
    "        print(\" - val_precision (per class):\")\n",
    "        print(_val_precision)\n",
    "        print(\" - val_recall (per class):\")\n",
    "        print(_val_recall)\n",
    "        print(\" \")\n",
    "        return\n",
    "\n",
    "time_summary = TimeSummary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an instance of the NN and compile it\n",
    "\n",
    "set_seed(321) #for repeatability\n",
    "\n",
    "model = NN_model()\n",
    "model.compile(\n",
    "    optimizer='Adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NN training and progress\n",
    "import keras.callbacks\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1, mode='min')\n",
    "mcp_save = ModelCheckpoint('.mdl_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=1, epsilon=1e-4, mode='min')\n",
    "\n",
    "summary = model.fit(\n",
    "    x_train_np, y_train_onehot,\n",
    "    batch_size=175,\n",
    "    epochs=100,\n",
    "    validation_split=0.25,\n",
    "    verbose=1,\n",
    "    validation_data=(x_test_np, y_test_onehot),#for f1 score\n",
    "    callbacks=[time_summary,earlyStopping, reduce_lr_loss]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify the NN accuracy\n",
    "score = model.evaluate(x_test_np, y_test_onehot, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the training summary to check the performace\n",
    "\n",
    "plot_training_summary(summary, time_summary) # attention: bug found after displaying the plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the built NN model and the calculated weights\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(home_folder + \"model_v1.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "model.save_weights(home_folder + \"model_v1.h5\")\n",
    "\n",
    "print(\"Saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
